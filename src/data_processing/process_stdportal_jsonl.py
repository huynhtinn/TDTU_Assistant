# src/data_processing/process_stdportal_jsonl.py

"""
Script x·ª≠ l√Ω data.jsonl (243 documents) v√† ƒë∆∞a v√†o ChromaDB
"""

import json
import os
from pathlib import Path
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain.schema import Document

def load_jsonl(file_path):
    """ƒê·ªçc file JSONL (m·ªói d√≤ng 1 JSON object)"""
    documents = []
    
    with open(file_path, 'r', encoding='utf-8') as f:
        for line in f:
            if line.strip():  # B·ªè qua d√≤ng tr·ªëng
                try:
                    data = json.loads(line)
                    documents.append(data)
                except json.JSONDecodeError as e:
                    print(f"‚ö†Ô∏è  L·ªói parse JSON: {e}")
                    continue
    
    return documents

def categorize_document(doc):
    """Ph√¢n lo·∫°i document theo source"""
    source = doc['metadata'].get('source', '').lower()
    
    # Ph√¢n lo·∫°i d·ª±a v√†o keywords
    if any(kw in source for kw in ['h·ªçc b·ªïng', 'khen th∆∞·ªüng', 'h·ªó tr·ª£']):
        return 'financial'
    elif any(kw in source for kw in ['ƒë·∫°o ƒë·ª©c', 'n·ªôi quy', 'quy t·∫Øc', 'vi ph·∫°m', 'k·ª∑ lu·∫≠t']):
        return 'student_life'
    elif any(kw in source for kw in ['t·ªët nghi·ªáp', 'r√®n luy·ªán', 'ƒëi·ªÉm']):
        return 'academic'
    else:
        return 'general'

def create_langchain_documents(jsonl_docs):
    """Chuy·ªÉn JSONL docs th√†nh LangChain Documents"""
    
    langchain_docs = []
    category_count = {'financial': 0, 'academic': 0, 'student_life': 0, 'general': 0}
    
    for doc in jsonl_docs:
        # L·∫•y n·ªôi dung
        content = doc.get('page_content', '')
        metadata = doc.get('metadata', {})
        
        # Ph√¢n lo·∫°i
        category = categorize_document(doc)
        category_count[category] += 1
        
        # T·∫°o Document
        langchain_doc = Document(
            page_content=content,
            metadata={
                'source': metadata.get('source', 'Unknown'),
                'category': category,
                'page': metadata.get('page', 1),
                'file_name': metadata.get('file_name', ''),
                'processed_date': metadata.get('processed_date', '')
            }
        )
        
        langchain_docs.append(langchain_doc)
    
    print(f"\nüìä Th·ªëng k√™ ph√¢n lo·∫°i:")
    for cat, count in category_count.items():
        print(f"   {cat}: {count} docs")
    
    return langchain_docs

def create_chromadb_by_category(documents, base_dir='data/processed'):
    """T·∫°o ChromaDB ri√™ng cho t·ª´ng category"""
    
    # Group documents by category
    categorized_docs = {
        'financial': [],
        'academic': [],
        'student_life': [],
        'general': []
    }
    
    for doc in documents:
        category = doc.metadata['category']
        categorized_docs[category].append(doc)
    
    # Kh·ªüi t·∫°o embedding model (dimension 384 - matching ChromaDB)
    print("\nüîß Kh·ªüi t·∫°o embedding model...")
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        model_kwargs={'device': 'cpu'},
        encode_kwargs={'normalize_embeddings': False}
    )
    
    # T·∫°o ChromaDB cho t·ª´ng category
    for category, docs in categorized_docs.items():
        if not docs:
            print(f"‚ö†Ô∏è  B·ªè qua {category}: kh√¥ng c√≥ docs")
            continue
        
        db_path = os.path.join(base_dir, f'{category}_db')
        
        print(f"\nüìÇ T·∫°o ChromaDB cho {category}...")
        print(f"   S·ªë documents: {len(docs)}")
        print(f"   ƒê∆∞·ªùng d·∫´n: {db_path}")
        
        # T·∫°o/c·∫≠p nh·∫≠t ChromaDB
        try:
            vectordb = Chroma.from_documents(
                documents=docs,
                embedding=embeddings,
                persist_directory=db_path,
                collection_name="langchain"
            )
            
            print(f"   ‚úÖ ƒê√£ l∆∞u {vectordb._collection.count()} vectors")
            
        except Exception as e:
            print(f"   ‚ùå L·ªói: {e}")

def main():
    """Main function"""
    
    print("=" * 60)
    print("PROCESS STDPORTAL JSONL ‚Üí ChromaDB")
    print("=" * 60)
    
    # 1. ƒê·ªçc JSONL
    jsonl_path = 'data/stdportal/data.jsonl'
    print(f"\nüìÇ ƒê·ªçc file: {jsonl_path}")
    
    if not os.path.exists(jsonl_path):
        print(f"‚ùå File kh√¥ng t·ªìn t·∫°i: {jsonl_path}")
        return
    
    jsonl_docs = load_jsonl(jsonl_path)
    print(f"‚úÖ ƒê·ªçc ƒë∆∞·ª£c {len(jsonl_docs)} documents")
    
    # 2. Chuy·ªÉn sang LangChain Documents
    print("\nüîÑ Chuy·ªÉn ƒë·ªïi sang LangChain Documents...")
    langchain_docs = create_langchain_documents(jsonl_docs)
    
    # 3. T·∫°o ChromaDB
    print("\nüíæ T·∫°o ChromaDB theo category...")
    create_chromadb_by_category(langchain_docs)
    
    print("\n" + "=" * 60)
    print("‚úÖ HO√ÄN TH√ÄNH!")
    print("=" * 60)
    print("\nüìå K·∫øt qu·∫£:")
    print("   - data/processed/financial_db/     (h·ªçc b·ªïng, khen th∆∞·ªüng)")
    print("   - data/processed/academic_db/      (t·ªët nghi·ªáp, r√®n luy·ªán)")
    print("   - data/processed/student_life_db/  (n·ªôi quy, ƒë·∫°o ƒë·ª©c)")
    print("   - data/processed/general_db/       (th√¥ng tin chung)")
    print("\nüéØ Agents gi·ªù c√≥ th·ªÉ truy v·∫•n 243 documents quy ch·∫ø!")

if __name__ == "__main__":
    main()
