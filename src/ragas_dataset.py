"""
ragas_dataset.py
================
B∆∞·ªõc 1: Thu th·∫≠p d·ªØ li·ªáu ƒë√°nh gi√° t·ª´ RAG system.

Script n√†y ƒë·ªçc eval_layer2.csv (question + ground_truth),
g·ªçi h·ªá th·ªëng RAG ƒë·ªÉ l·∫•y answer v√† contexts,
v√† l∆∞u dataset ho√†n ch·ªânh ra file JSON.

Ch·∫°y:
    python eval/ragas_dataset.py              # To√†n b·ªô dataset (99 c√¢u)
    python eval/ragas_dataset.py --limit 10   # Ch·ªâ 10 c√¢u ƒë·∫ßu (demo nhanh)
    python eval/ragas_dataset.py --start 5 --limit 10  # C√¢u 5‚Üí14
"""

import os
import sys
import json
import argparse
import time
from datetime import datetime

# ‚îÄ‚îÄ Setup paths ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
app_path = os.path.join(project_root, "src", "app")

for path in [app_path, project_root]:
    if path not in sys.path:
        sys.path.insert(0, path)

# ‚îÄ‚îÄ Import RAG system ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
print("‚è≥ ƒêang kh·ªüi ƒë·ªông h·ªá th·ªëng RAG...")
try:
    from main import process_query_with_context
    print("‚úÖ H·ªá th·ªëng RAG ƒë√£ s·∫µn s√†ng.\n")
except Exception as e:
    print(f"‚ùå Kh√¥ng th·ªÉ kh·ªüi ƒë·ªông RAG system: {e}")
    sys.exit(1)

# ‚îÄ‚îÄ Paths ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
CSV_FILE    = os.path.join(project_root, "data", "eval", "eval_dataset.csv")
OUTPUT_FILE = os.path.join(project_root, "data", "eval", "ragas_dataset.json")


def load_csv(filepath: str) -> list[dict]:
    """
    ƒê·ªçc eval_layer2.csv ‚Üí list of {question_id, question, ground_truth}.
    
    Format m·ªõi (comma-separated, 3 c·ªôt):
        question_id, question_text, ground_truth
    """
    import csv
    samples = []
    with open(filepath, encoding="utf-8", newline="") as f:
        reader = csv.DictReader(f)
        for row in reader:
            question     = row.get("question_text", "").strip()
            ground_truth = row.get("ground_truth", "").strip()
            question_id  = row.get("question_id", "").strip()
            if question and ground_truth:
                samples.append({
                    "question_id":  question_id,
                    "question":     question,
                    "ground_truth": ground_truth,
                })
    return samples


def collect_rag_outputs(samples: list[dict], delay: float = 1.0) -> list[dict]:
    """
    G·ªçi RAG system cho t·ª´ng c√¢u h·ªèi,
    thu th·∫≠p answer v√† contexts.
    
    Args:
        samples : list[{question, ground_truth}]
        delay   : s·ªë gi√¢y ngh·ªâ gi·ªØa c√°c l·∫ßn g·ªçi (tr√°nh rate limit)
    Returns:
        list[{question, answer, contexts, ground_truth}]
    """
    dataset = []
    total   = len(samples)

    print(f"üìã B·∫Øt ƒë·∫ßu thu th·∫≠p RAG outputs ({total} c√¢u h·ªèi)...")
    print("‚îÄ" * 60)

    for i, sample in enumerate(samples, 1):
        question     = sample["question"]
        ground_truth = sample["ground_truth"]

        print(f"[{i:3}/{total}] ‚ùì {question[:70]}{'...' if len(question)>70 else ''}")

        try:
            answer, ctx_docs = process_query_with_context(question)

            # Chu·∫©n h√≥a contexts th√†nh list[str]
            contexts = []
            for doc in ctx_docs:
                if hasattr(doc, "page_content"):      # LangChain Document
                    contexts.append(doc.page_content.strip())
                elif isinstance(doc, str) and doc.strip():
                    contexts.append(doc.strip())

            entry = {
                "question_id":  sample.get("question_id", ""),
                "question":     question,
                "answer":       answer,
                "contexts":     contexts,
                "ground_truth": ground_truth,
            }
            dataset.append(entry)

            ctx_count = len(contexts)
            print(f"       ‚úÖ answer ({len(answer)} chars) | contexts: {ctx_count} chunks")

        except Exception as e:
            print(f"       ‚ùå L·ªói: {e}")
            dataset.append({
                "question_id":  sample.get("question_id", ""),
                "question":     question,
                "answer":       f"ERROR: {e}",
                "contexts":     [],
                "ground_truth": ground_truth,
            })

        # Delay gi·ªØa c√°c l·∫ßn g·ªçi API
        if i < total:
            time.sleep(delay)

    return dataset


def save_dataset(dataset: list[dict], output_path: str):
    """L∆∞u dataset ra JSON v·ªõi metadata."""
    os.makedirs(os.path.dirname(output_path), exist_ok=True)

    output = {
        "metadata": {
            "created_at":   datetime.now().isoformat(),
            "total_samples": len(dataset),
            "description":  "RAGAS evaluation dataset for TDTU RAG system",
            "fields":       ["question", "answer", "contexts", "ground_truth"],
        },
        "samples": dataset,
    }

    with open(output_path, "w", encoding="utf-8") as f:
        json.dump(output, f, ensure_ascii=False, indent=2)

    print(f"\n‚úÖ ƒê√£ l∆∞u {len(dataset)} samples ‚Üí {output_path}")


def main():
    parser = argparse.ArgumentParser(
        description="Thu th·∫≠p RAG outputs ƒë·ªÉ t·∫°o RAGAS evaluation dataset."
    )
    parser.add_argument(
        "--limit", type=int, default=None,
        help="Gi·ªõi h·∫°n s·ªë c√¢u (m·∫∑c ƒë·ªãnh: t·∫•t c·∫£)"
    )
    parser.add_argument(
        "--start", type=int, default=0,
        help="B·∫Øt ƒë·∫ßu t·ª´ index th·ª© bao nhi√™u (m·∫∑c ƒë·ªãnh: 0)"
    )
    parser.add_argument(
        "--delay", type=float, default=1.0,
        help="Th·ªùi gian ngh·ªâ gi·ªØa c√°c API call (gi√¢y, m·∫∑c ƒë·ªãnh: 1.0)"
    )
    parser.add_argument(
        "--output", type=str, default=OUTPUT_FILE,
        help="ƒê∆∞·ªùng d·∫´n file JSON output"
    )
    args = parser.parse_args()

    # ƒê·ªçc CSV
    print(f"üìÇ ƒê·ªçc dataset: {CSV_FILE}")
    all_samples = load_csv(CSV_FILE)
    print(f"   ‚Üí T√¨m th·∫•y {len(all_samples)} c√¢u h·ªèi.\n")

    # Slice theo start/limit
    samples = all_samples[args.start:]
    if args.limit is not None:
        samples = samples[:args.limit]

    print(f"üìä S·∫Ω x·ª≠ l√Ω {len(samples)} c√¢u (start={args.start}, limit={args.limit})\n")

    # Thu th·∫≠p
    t0      = time.time()
    dataset = collect_rag_outputs(samples, delay=args.delay)
    elapsed = time.time() - t0

    # L∆∞u
    save_dataset(dataset, args.output)

    # T√≥m t·∫Øt
    successful   = sum(1 for d in dataset if not d["answer"].startswith("ERROR"))
    has_contexts = sum(1 for d in dataset if d["contexts"])
    print("\n" + "=" * 60)
    print("üìä T·ªîNG K·∫æT THU TH·∫¨P D·ªÆ LI·ªÜU")
    print("=" * 60)
    print(f"  T·ªïng c√¢u h·ªèi       : {len(dataset)}")
    print(f"  Th√†nh c√¥ng         : {successful}/{len(dataset)}")
    print(f"  C√≥ contexts        : {has_contexts}/{len(dataset)}")
    print(f"  Th·ªùi gian ch·∫°y     : {elapsed:.1f}s")
    print(f"  Output             : {args.output}")
    print("=" * 60)
    print("\nüí° B∆∞·ªõc ti·∫øp theo: python eval/ragas_evaluate.py")


if __name__ == "__main__":
    main()
